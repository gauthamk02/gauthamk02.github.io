<!DOCTYPE html>
<html lang="en-us">
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    <title>Intro to Maths of Neural Networks | Gautham Krishnan</title>

    <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">    
<meta name="viewport" content="width=device-width,minimum-scale=1">
<meta name="description" content="Deep Learning and AI have gained a lot of popularity because of the recent advancements in the field showing their capabilities for tasks like image detection, chat completion, image generation etc. At the base of all these models is Artifical Neural Networks(ANN). This article provides a basic understanding of maths and theoretical working of these networks.
Most of the maths present in this article comes under Linear Algebra and Calculus. Anyone who has done an undergrad course or two on these topics will be familiar with the concepts explained in the article.">
<meta name="generator" content="Hugo 0.147.9">


  <META NAME="ROBOTS" CONTENT="NOINDEX, NOFOLLOW">


<link rel="stylesheet" href="/css/style.css">


  
    
    <link rel="stylesheet" href="http://localhost:1313/css/custom.css">
  


<link rel="shortcut icon" href="/images/favicon.ico" type="image/x-icon" />

 
  
      <script async src="https://www.googletagmanager.com/gtag/js?id=G-LBPYE8PRE4"></script>
      <script>
        var doNotTrack = false;
        if ( false ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-LBPYE8PRE4');
        }
      </script>



  
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/katex.min.css" integrity="sha384-RZU/ijkSsFbcmivfdRBQDtwuwVqK7GMOw6IMvKyeWL2K5UAlyp6WonmB8m7Jd0Hn" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/katex.min.js" integrity="sha384-pK1WpvzWVBQiP0/GjnvRxV4mOb0oxFuyRxJlk6vVw146n3egcN5C925NCP7a7BY8" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/contrib/auto-render.min.js" integrity="sha384-vZTG03m+2yp6N6BNi5iM4rW4oIwk5DfcNdFfxkk9ZWpDriOkXX8voJBFrAO7MpVl" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>





  </head>

  <body>
    <nav class="navigation">
	
		<a href="/"> <span class="arrow">‚Üê</span>Home</a>
	
	
	
	<a href="/about">About</a>

	
		<a href="/posts">Posts</a>
	
		<a href="/bookshelf">Bookshelf</a>
	
		<a href="/finds">Finds</a>
	

	
	
		<a class="CVbutton" href="https://drive.google.com/file/d/1m1Tbe-himD18DG0xv2gzxOKHdKMI3H5-/view?usp=sharing">CV</a>
	

	
	  <a class="button" href="">Subscribe</a>
	
</nav>


    <main class="main">
      

<section id="single">
    <h1 class="title">Intro to Maths of Neural Networks</h1>

    <div class="tip">
        <time datetime="2023-09-04 00:00:00 &#43;0000 UTC">Sep 4, 2023</time>
        <span class="split">
          ¬∑
        </span>
        <span>
          2207 words
        </span>
        <span class="split">
          ¬∑
        </span>
        <span>
          11 minute read
        </span>
    </div>

    
    
        
  
    <aside class="toc">
      <details>
          <summary>Table of Contents
          </summary>
          <div>
              <nav id="TableOfContents">
  <ul>
    <li><a href="#what-is-a-neural-network">What is a Neural Network?</a></li>
    <li><a href="#why-neural-networks-work-neural-networks-as-function-approximators">Why Neural Networks work? Neural Networks as function approximators</a></li>
    <li><a href="#matrices-and-anns">Matrices and ANNs</a></li>
    <li><a href="#activations-and-non-linearity">Activations and Non-Linearity</a></li>
    <li><a href="#errors-and-cost-function">Errors and Cost Function</a></li>
    <li><a href="#backpropagation-and-gradient-descent">Backpropagation and Gradient Descent</a>
      <ul>
        <li><a href="#backpropagation">Backpropagation</a></li>
        <li><a href="#gradient-descent">Gradient Descent</a></li>
      </ul>
    </li>
    <li><a href="#training-anns">Training ANNs</a></li>
    <li><a href="#references">References</a></li>
  </ul>
</nav>
          </div>
      </details>
    </aside>
  


    


    <div class="content">
      <p>Deep Learning and AI have gained a lot of popularity because of the recent advancements in the field showing their capabilities for tasks like image detection, chat completion, image generation etc. At the base of all these models is Artifical Neural Networks(ANN). This article provides a basic understanding of maths and theoretical working of these networks.</p>
<p>Most of the maths present in this article comes under Linear Algebra and Calculus. Anyone who has done an undergrad course or two on these topics will be familiar with the concepts explained in the article.</p>
<h2 id="what-is-a-neural-network">What is a Neural Network? <a href="#what-is-a-neural-network" class="anchor">üîó</a></h2><p>A neural network or an Artifical Neural Network is a collection of nodes that are arranged in layers in such a way that it loosely resembles the connections of neurons in a brain where each node represents a neuron. Neural Networks have an input layer, one or more hidden layers and an output layer.</p>
<center>
    <img src=images/simple-feed-forward-ann.png width=700>
    <figcaption><i>A simple feed forward ANN with an input layer, 3 hidden layers and an outpt layer</i></figcaption> <br>
</center>
<p>In this article we will only be dealing with the most basic architecture of an ANN, i.e a feed-forward neural network. Every node in a layer of a feed-forward neural network is connected to all the nodes in the previous layer and the input data moves from one layer to the next one sequentially.</p>
<h2 id="why-neural-networks-work-neural-networks-as-function-approximators">Why Neural Networks work? Neural Networks as function approximators <a href="#why-neural-networks-work-neural-networks-as-function-approximators" class="anchor">üîó</a></h2><p>Neural Networks are capable of approximating a function to any arbitrary precision. A function can be anything from calculating the probablity of rain given the weather data, identifying if the given picture is that of cat or a dog, or whats the next best move to play in chess.</p>
<p>This property of ANNs is supported by the <em>Universal Approximation Theorem</em>^[1] which states that a Neural Network with as small as one hidden layer can approximate a function to an arbitrary degree of precision provided sufficient number of nodes are present in the hidden layer. This theorem provided the mathematical foundation for why Neural Networks work.</p>
<p>However, the theorem only states that neural networks are capable of approximating functions. For the network to work properly for a given task, the neurons have to contain the correct weights which will be learned during the training process.</p>
<h2 id="matrices-and-anns">Matrices and ANNs <a href="#matrices-and-anns" class="anchor">üîó</a></h2><p>Neural Networks in its most basic form can be represented as a collection of matrix operations and functions.</p>
<center>
    <img src=images/single-layer-with-weights.png height=400>
</center>
<br>
<p>Consider the above neural network with \(n\) input nodes and \(m\) nodes in the first layer. Each neuron in the first layer is connected to all the inputs nodes and each connection has a weight of their own. Therefore the output of the first neuron can be represented as:</p>
<p>$$
a_{1}^{(1)} = \sigma\left(w_{1,0}a_{1}^{(0)} + w_{1,1}a_{2}^{(0)} + \ldots + w_{1,n}a_{n}^{(0)} + b_1^{(0)}\right) \
=\sigma\left(\sum_{i=1}^{n} w_{1,i}a_{i}^{(0)} + b_1^{(0)}\right)
$$</p>
<p>where \(b_1^{(0)}\) is the bias of the first neuron and \(\sigma\) is the activation function. We will come to activation functions and why they are necessary soon. Now let all the neurons in the first layer be represented by the column vector \(a^{(1)}\). Then \(a^{(1)}\) can be written as:</p>
<center>
    <img src=images/matrix-equation.png width=600>
</center>
<br>
<p>To simply it further,</p>
<p>$$
a^{(1)} = \sigma\left(\mathbf{W}^{(1)} a^{(0)}+\mathbf{b}^{(1)}\right),\quad \mathbf{W}^{(1)} \in \mathbb{R}^{m\times n}, \mathbf{b}^{(1)} \in \mathbb{R}^{m\times 1}
$$</p>
<p>\(\mathbf{W}^{(1)}\) is all the combined weight and \(\mathbf{b}^{(1)}\) is the combined biases of the first layer.</p>
<center>
    <img src=images/simple-ann.png width=500>
</center>
<br>
<p>Consider a simple ANN like the one given above. Mathematically it can be written as:</p>
<p>$$
\mathbf{F(X)} = \sigma\left(\mathbf{W}^{(3)}*\sigma\left(\mathbf{W}^{(2)}*\sigma\left(\mathbf{W}^{(1)}*\mathbf{X}^{(0)}+\mathbf{b}^{(1)}\right) + \mathbf{b}^{(2)}\right) + \mathbf{b}^{(3)}\right)
$$</p>
<h2 id="activations-and-non-linearity">Activations and Non-Linearity <a href="#activations-and-non-linearity" class="anchor">üîó</a></h2><p>Contructing Neural Networks where the vaues of nodes are calculated using the weighted sum of values from the previous layer using matrix multiplications has a major draw-back. Weighted sum and matrix multplications are linear functions and a combination of linear functions would result in another linear function. This means that the neural network consisting multiple hidden layers is equivalent to shallow network having one hidden layer. Such a network can only learn a linear decision boundary. Activation functions allow the model input to be mapped to the output in non-linear ways. This is necessay for any network that has to deal with non trivial tasks.</p>
<p>A decision boundary is the line that separates the input values to dfferent output classes. A linear decision bondary would be a line in 2D and a hyperplane in 3D.</p>
<center>
    <img src=images/linear-and-nonlinear-function-graph.png width=700>
</center>
<br>
<p>Consider the above two graphs where \(x_1\) and \(x_2\) are the input data and red, blue are two different classes. On the graph on the left, the data is arranged in such a way that it can be separated by a stright line, i.e it is linearly separable. A model for classifying such data can be built without using activation functions and the model will learn a decision boundary similar to the one given in the graph. However, the Graph on the right consists of data-points that is arrranged in a non-linearly separable way. To make a model capable of classifying the points in this graph accurately, we will have to use activation functions.</p>
<p>Still confused? Visit <a href="https://playground.tensorflow.org/" target="_blank" rel="noopener">Tensorflow Playground</a>, select a non-linearly separable data and try to train the model using different activation functions. The linear activation function is an identity function \(\sigma(x) = x\) or any other linear function and wont allow the model to learn a non-linear decision boundary. The model will only be able to classify the non-linear data if a non-linear activation function is used.</p>
<p>There are different activation functions that are used for constructing ANNs. Most of the time all the hidden layers will have the same activation and output layer, a different one. Some of the common non-linear activation functions are:</p>
<ol>
<li>
<p><strong>Sigmoid function</strong>
The sigmoid function \(\sigma:\mathbb{R}\rightarrow[0, 1]\) takes in a real numbered value and maps it to between 0 and 1 in such a way that \(\lim_{x\to\infty} \sigma(x) = 1\) and \(\lim_{x\to-\infty} \sigma(x) = 0\). The equation of Sigmoid funcion is
$$\sigma(x) = \frac{1}{1+ e^{-x}}$$</p>
<p>Sigmmoid function is usually used in the output layer of classification models where you want to get the output as a probability value between 0 and 1.</p>
 <center>
     <img src=images/sigmoid.png width=300>
 </center>
 <br>
</li>
<li>
<p><strong>Tanh</strong>
Also known as the hyperbolic tangent function, the function \(\sigma:\mathbb{R}\rightarrow[-1, 1]\) takes in real values and maps it to [-1, 1]. Its equation is given by
$$\sigma(x) = \frac{e^{x}+e^{-x}}{e^{x}- e^{-x}}$$</p>
 <center>
     <img src=images/tanh.png width=300>
 </center>
 <br>
</li>
<li>
<p><strong>ReLU</strong>
ReLU function \(\sigma:\mathbb{R}\rightarrow[0, \infty]\) maps all the real numbers to between 0 to \(\infty\). ReLU can be written as
$$\sigma(x) = max(0, x)$$</p>
 <center>
     <img src=images/relu.png width=300>
 </center>
 <br>
</li>
</ol>
<h2 id="errors-and-cost-function">Errors and Cost Function <a href="#errors-and-cost-function" class="anchor">üîó</a></h2><p>We&rsquo;ve seen how ANNs are constructed and how data flows through them. However for a neural network to give the output we want, they have to be trained. Here are some notations which we will be using from now on to represent the training data and the output given by the neural network.</p>
<p>\(\mathbf{X}\) = Input data for training
\(\mathbf{Y}\) = True output value for input data
\(\mathbf{\hat{Y}}\) = Predicted output value for input data
\(\mathbf{n}\) = Number of samples of training data</p>
<p>\(x_i,y_i,\hat{y}_i\) are the \(i^{th}\) elements of \(\mathbf{X}\), \(\mathbf{Y}\) and \(\mathbf{\hat{Y}}\) respectively.</p>
<p>To train the model we calculate the error and try to minimize it by updating the values of the weights iteratively. Error represents by how much the output of the network deviates from the true value. The function used to calculate the error is called a <em>Cost function</em>. One of the popular cost functions used for training neural networks is <em>MSE</em> or <em>Mean Squared Error</em>. MSE is calculated as</p>
<p>$$
\mathbf{MSE} = \frac{(\mathbf{\hat{Y}} - \mathbf{Y})^2}{2n} = \frac{1}{2n}\sum_{i = 0}^n(\hat{y}_i - y_i)^2
$$</p>
<p>Some of the other cost functions used are Cross-Entropy, Binary Cross-Entropy, Intersection over Union etc.</p>
<h2 id="backpropagation-and-gradient-descent">Backpropagation and Gradient Descent <a href="#backpropagation-and-gradient-descent" class="anchor">üîó</a></h2><h3 id="backpropagation">Backpropagation <a href="#backpropagation" class="anchor">üîó</a></h3><p>Backprogation is calculating the gradient of the error with respect to the individual weights of the nodes. It is called backpropagation as it starts from the last layers and iteratively calculates the the gradients till the first layer. Back propagation along with gradient descent is used to update the weights of the neurons to train the model to reduce the error to a minimum.</p>
<center>
    <img src=images/simple-ann.png width=500>
</center>
<br>
<p>Consider the previously shown Neural Network. The MSE error of this network is given by
$$
\mathbf{Error(E)} = \frac{1}{2n}\sum_{i = 0}^n(\hat{y}_i - y_i)^2, \quad n=2
$$</p>
<p>and error due to the \(i^{th}\) output node can be written as
$$
\mathbf{E} = \frac{1}{2}(\hat{y}_i - y)^2
$$</p>
<p>To update the weights of the neurons we have to find the gradient of the error with respect to it which can be calculated using chain rule.
$$
\frac{\partial \mathbf{E}}{\partial w_{ij}^k} = \frac{\partial \mathbf{E}}{\partial o_j^k}\frac{\partial o_j^k}{\partial w_{ij}^k} \qquad (1)
$$</p>
<p>\(o_j^k\) is the summed input (\(input * weights + bias\)) of the \(j^{th}\) neuron in layer \(k\).</p>
<p>$$
\frac{\partial o_j^k}{\partial w_{ij}^k} = \frac{\partial \sum_{l=1}^{n}w_{lj}^k*a_l^{k-1} + b^k}{\partial w_{ij}^k} = a_i^{k-1}
$$</p>
<p>\(\dfrac{\partial \mathbf{E}}{\partial o_j^k}\) is called the error of \(j^{th}\) node at level \(k\) and is denoted by \(\delta_j^k\).</p>
<p>Therefore, Eq (1) can be written as \(\dfrac{\partial \mathbf{E}}{\partial w_{ij}^k} = \delta_j^k*a_i^{k-1}\)</p>
<p>The value \(\delta_j^k\) depends on whether if it belongs to an output node or a hidden node.</p>
<p><strong>For an output nodes:</strong>
The error value of the nodes in the output layer can be calculated direclty by taking the derivative of the cost function with respect to the input of the node.</p>
<p>$$
\mathbf{E} = \dfrac{1}{2}(\hat{y} - y)^2 =\dfrac{1}{2}(\sigma(o_j) - y)^2 = \dfrac{1}{2}(a_j - y)^2
$$</p>
<p>So \(\delta_j^k = \dfrac{\partial \mathbf{E}}{\partial o_j^k} = \dfrac{\partial \mathbf{E}}{\partial a_j}\dfrac{\partial a_j} {\partial o_j}=(a_j - y)\sigma&rsquo;(o_j)\)</p>
<p><strong>For hidden nodes:</strong>
Unlike an output node, a hidden node&rsquo;s error term will be affected by all the nodes in the layer after it. As in it will be a weighted sum of all the errors of the nodes in the succeeding layer. We can use chain rule for multivariant functions for formulating the equation for finding the error of hidden nodes.</p>
<p>$$
\delta_j^k=\dfrac{\partial \mathbf{E}}{\partial o_j^k}
= \sum_{l=1}^n \frac{\partial \mathbf{E}}{\partial o_l^{k + 1}}\frac{\partial o_l^{k + 1}}{\partial o_j^k} \
= \sum_{l=1}^n \delta_l^{k+1}\frac{\partial o_j^{k + 1}}{\partial o_j^k}
$$</p>
<p>\(o_j^{k + 1}\) is given by \(o_j^{k + 1} = \sum_{i=0}w_{ij}^{k+1}\sigma(o_{j}^k)\)</p>
<p>Therefore,
$$
\frac{\partial o_j^{k + 1}}{\partial o_j^k} = w_{ij}^{k+1}\sigma&rsquo;(o_{j}^k)
$$</p>
<p>Combining them \(\delta_j^k\) can be written as,
$$
\delta_j^k=\sum_{l=1}^n \delta_l^{k+1}w_{lj}^{k+1}\sigma&rsquo;(o_{j}^k) \
= \sigma&rsquo;(o_{j}^k)\sum_{l=1}^n \delta_l^{k+1}w_{lj}^{k+1}
$$</p>
<p>Using all the above equations, the general formula for \(\dfrac{\partial \mathbf{E}}{\partial w_{ij}^k}\)is</p>
<!-- $$
\usepackage{amsmath}
\frac{\partial \mathbf{E}}{\partial w_{ij}^k} = 
\begin{cases}
(\hat{y} - y)\sigma'(o_j)a_i^{k-1},& \text{for output nodes} \\
\sigma'(o_{j}^k)a_i^{k-1}\sum_{l=1}^n \delta_l^{k+1}w_{lj}^{k+1},& \text{for hidden nodes}
\end{cases}
$$ -->
<center>
    <img src=images/backpropagation-full-equation.png width=500>
</center>
<p>It can be seen that computing the value of \(\delta_l^{k}\) depends on the errors of the succeeding nodes, the computed values are cached and reused as the compuation of gradients start from the output layers and proceed to the initial layers.</p>
<blockquote>
<p>Backpropagation is the most of math intensive and confusing part for simple ANN&rsquo;s such as the ones discussed here. A lot of inspiration on formulating the equations and ordering them was taken from the well written blog <a href="https://brilliant.org/wiki/backpropagation/" target="_blank" rel="noopener">Backpropagation by John McGonagle, George Shaikouski, Christopher Williams, and 3 others</a> present at <a href="https://brilliant.org/" target="_blank" rel="noopener">Brilliant</a>.</p></blockquote>
<h3 id="gradient-descent">Gradient Descent <a href="#gradient-descent" class="anchor">üîó</a></h3><p>Gradient descent is a first-order iterative optimisation algorithm that is commonly used for training machine learning models. The algorithm can be used for finding the local minima of a differentiable function &mdash; a cost function in the case of machine learning models.</p>
<center>
    <img src=images/gradient-descent.png width=300>
    <figcaption><i>Gradient descent on a simple convex function</i></figcaption>
</center>
<br>
<p>Gradient descent works by calculating the gradients of the output of the function to minimise with respect to its parameters. The parameter is then updated in the direction opposite of that of the calculated gradient after multiplying it by a constant called the learning rate(\(\eta\)). By doing this multiple times, the parameters of the function will have values that will reduce its output to a local minimum.</p>
<p>$$
x_{n+1} = x_n - \eta\nabla F
$$</p>
<p>It is necessary to choose an appropriate learning rate as a learning rate that is too large would cause divergence, where the model over shoots the minima. On the othe hand, a learning rate that is too small would be slow to converge.</p>
<p>For ANN, we can write the gradient descent algorithm as,
$$
w_{n+1} = w_n - \eta\frac{\partial \mathbf{E}}{\partial w_{n}}
$$</p>
<p>In the case of Machine Learning models like Linear Regression, the cost function MSE is a convex function and Gradient descent is guaranteed to find the optimal solution, i.e the global minima, provided a reasonably good learning rate is used. However, in the case of ANNs the cost function MSE is not convex and wll have many local minima to which gradient descent will move to depends on factors like weights initialisation and learning rate.</p>
<h2 id="training-anns">Training ANNs <a href="#training-anns" class="anchor">üîó</a></h2><p>Keeping the theoretical part aside, many other factors also come into consideration when training an ANN. Such as quality of the dataset, choosing the right model architecture and hyperparameters, the training cost and time etc. Assuming all these factors are accounted for, the steps for training a neural network using a modern Deep Learning framework such as <a href="https://pytorch.org/" target="_blank" rel="noopener">PyTorch</a> or <a href="https://www.tensorflow.org/" target="_blank" rel="noopener">Tensorflow</a> will be as given below:</p>
<ol>
<li>Load the data - The data along with the labels for training the modelis loaded</li>
<li>Forward pass - The data is passed through the model and output is stored</li>
<li>Calculating Loss - The output is compared with the true labels using cost function and the loss or error is calculated</li>
<li>Backpropagation - The gradient of all the nodes is calculated using backpropagation</li>
<li>Gradient Descent - The weights are updated using Gradient Descent algorithm</li>
</ol>
<p>These steps will be repeated for a predefned number of times or until convergence.</p>
<h2 id="references">References <a href="#references" class="anchor">üîó</a></h2><p>[1] Hornik, K., Stinchcombe, M. and White, H. (1989) ‚ÄòMultilayer feedforward networks are universal approximators‚Äô, Neural Networks, 2(5), pp. 359‚Äì366. Available at: <a href="https://doi.org/10.1016/0893-6080%2889%2990020-8" target="_blank" rel="noopener">https://doi.org/10.1016/0893-6080(89)90020-8</a>.</p>

    </div>

    
    
    
  <div id="comment">
    
    
  </div>


</section>


    </main>
    
    <footer id="footer">
    

    <div class="copyright">
    
       ¬© Copyright 
       2025 
       <span class="split">
        <svg fill="#bbbbbb" width="15" height="15" version="1.1" id="heart-15" xmlns="http://www.w3.org/2000/svg" width="15px" height="15px" viewBox="0 0 15 15">
  <path d="M13.91,6.75c-1.17,2.25-4.3,5.31-6.07,6.94c-0.1903,0.1718-0.4797,0.1718-0.67,0C5.39,12.06,2.26,9,1.09,6.75&#xA;&#x9;C-1.48,1.8,5-1.5,7.5,3.45C10-1.5,16.48,1.8,13.91,6.75z"/>
</svg>
       </span>
       Gautham Krishnan
    
    </div>

    
</footer>



  </body>
</html>
